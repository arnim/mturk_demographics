{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics API\n",
    "\n",
    "Below we have the code that retrieves the data from the  Mechanical Turk Tracker Demographics API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# This function takes as input the response for a single survey, and transforms it into a flat dictionary\n",
    "def flatten(item):\n",
    "    fmt = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "    \n",
    "    hit_answer_date = datetime.strptime(item[\"date\"], fmt)\n",
    "    hit_creation_str = item.get(\"hitCreationDate\")\n",
    "    \n",
    "    if hit_creation_str is None: \n",
    "        hit_creation_date = None \n",
    "        diff = None\n",
    "    else:\n",
    "        hit_creation_date = datetime.strptime(hit_creation_str, fmt)\n",
    "        # convert to unix timestamp\n",
    "        hit_date_ts = time.mktime(hit_creation_date.timetuple())\n",
    "        answer_date_ts = time.mktime(hit_answer_date.timetuple())\n",
    "        diff = int(answer_date_ts-hit_date_ts)\n",
    "    \n",
    "    result = {\n",
    "        \"worker_id\": str(item[\"workerId\"]),\n",
    "        \"gender\": str(item[\"answers\"][\"gender\"]).lower(),\n",
    "        \"household_income\": str(item[\"answers\"][\"householdIncome\"]),\n",
    "        \"educational_level\": str(item[\"answers\"].get(\"educationalLevel\")),\n",
    "        \"household_size\": str(item[\"answers\"][\"householdSize\"]),\n",
    "        \"marital_status\": str(item[\"answers\"].get(\"maritalStatus\")),\n",
    "        \"languages_spoken\": str(item[\"answers\"].get(\"languagesSpoken\")),\n",
    "        \"time_spent_on_mturk\": str(item[\"answers\"].get(\"timeSpentOnMturk\")),\n",
    "        \"weekly_income_from_mturk\": str(item[\"answers\"].get(\"weeklyIncomeFromMturk\")),\n",
    "        \"year_of_birth\": int(item[\"answers\"][\"yearOfBirth\"]),\n",
    "        \"location_city\": str(item.get(\"locationCity\")),\n",
    "        \"location_region\": str(item.get(\"locationRegion\")),\n",
    "        \"location_country\": str(item[\"locationCountry\"]),\n",
    "        \"hit_answered_date\": hit_answer_date,\n",
    "        \"hit_creation_date\": hit_creation_date,\n",
    "        \"post_to_completion_secs\": diff\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved  5000  responses\n",
      "Total of  5000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  10000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  15000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  20000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  25000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  30000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  35000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  40000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  45000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  50000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  55000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  60000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  65000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  70000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  75000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  80000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  85000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  90000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  95000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  100000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  105000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  110000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  115000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  120000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  125000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  130000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  135000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  140000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  145000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  150000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  155000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  160000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  165000  responses in our data\n",
      "Retrieved  5000  responses\n",
      "Total of  170000  responses in our data\n",
      "Retrieved  2075  responses\n",
      "Total of  172075  responses in our data\n"
     ]
    }
   ],
   "source": [
    "# The code below retrieves all the responses from the Demographics API\n",
    "# Since we cannot get all the responses at once, we fetch a few thousand\n",
    "# records at a time, until fetching them all\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "limit = 5000\n",
    "\n",
    "# The API call that returns the last survey responses\n",
    "baseurl = \"https://mturk-surveys.appspot.com/\" + \\\n",
    "    \"_ah/api/survey/v1/survey/demographics/answers?limit=\" + str(limit)\n",
    "\n",
    "# This is the cursor variable, used to retrieve more pages of results\n",
    "nextPageToken = None\n",
    "\n",
    "# We store the results in this list\n",
    "results = []\n",
    "\n",
    "while True:\n",
    "    if nextPageToken == None:\n",
    "        url = baseurl\n",
    "    else:\n",
    "        url = baseurl + \"&cursor=\" + nextPageToken\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        data = json.loads(resp.text)\n",
    "        items = data.get(\"items\")\n",
    "        if items == None:\n",
    "            break\n",
    "        print(\"Retrieved \", len(items), \" responses\")\n",
    "        responses = [flatten(item) for item in items]\n",
    "        results.extend(responses)\n",
    "        print(\"Total of \", len(results), \" responses in our data\")\n",
    "    else:\n",
    "        print(\"Something went wrong with the network call\")\n",
    "\n",
    "    nextPageToken = data.get(\"nextPageToken\")\n",
    "    if nextPageToken == None:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172075\n"
     ]
    }
   ],
   "source": [
    "# Let's print the total number of retrieved responses\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "# Let's save the file as a CSV\n",
    "df.to_csv(\"mturk_surveys_extended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English', 'English,French', 'English', ..., 'None', 'None',\n",
       "       'None'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.languages_spoken.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'Abkhazian',\n",
       " 'Afar',\n",
       " 'Afrikaans',\n",
       " 'Albanian',\n",
       " 'Amharic',\n",
       " 'Arabic',\n",
       " 'Armenian',\n",
       " 'Assamese',\n",
       " 'Azerbaijani',\n",
       " 'Bashkir',\n",
       " 'Basque',\n",
       " 'Bengali',\n",
       " 'Bihari',\n",
       " 'Bulgarian',\n",
       " 'Burmese',\n",
       " 'Byelorussian',\n",
       " 'Cambodian',\n",
       " 'Catalan',\n",
       " 'Chinese',\n",
       " 'Croatian',\n",
       " 'Czech',\n",
       " 'Danish',\n",
       " 'Dutch',\n",
       " 'English',\n",
       " 'Esperanto',\n",
       " 'Estonian',\n",
       " 'Fiji',\n",
       " 'Finnish',\n",
       " 'French',\n",
       " 'Frisian',\n",
       " 'Gaelic',\n",
       " 'Galician',\n",
       " 'Georgian',\n",
       " 'German',\n",
       " 'Greek',\n",
       " 'Guarani',\n",
       " 'Gujarati',\n",
       " 'Hausa',\n",
       " 'Hebrew',\n",
       " 'Hindi',\n",
       " 'Hungarian',\n",
       " 'Icelandic',\n",
       " 'Indonesian',\n",
       " 'Interlingua',\n",
       " 'Irish',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Javanese',\n",
       " 'Kannada',\n",
       " 'Kashmiri',\n",
       " 'Korean',\n",
       " 'Kurdish',\n",
       " 'Laothian',\n",
       " 'Latin',\n",
       " 'Latvian',\n",
       " 'Lithuanian',\n",
       " 'Macedonian',\n",
       " 'Malagasy',\n",
       " 'Malay',\n",
       " 'Malayalam',\n",
       " 'Maltese',\n",
       " 'Marathi',\n",
       " 'Mongolian',\n",
       " 'Nepali',\n",
       " 'Norwegian',\n",
       " 'Oriya',\n",
       " 'Pashto',\n",
       " 'Persian',\n",
       " 'Polish',\n",
       " 'Portuguese',\n",
       " 'Punjabi',\n",
       " 'Romanian',\n",
       " 'Russian',\n",
       " 'Samoan',\n",
       " 'Sanskrit',\n",
       " 'Serbian',\n",
       " 'Serbo-Croatian',\n",
       " 'Shona',\n",
       " 'Sindhi',\n",
       " 'Singhalese',\n",
       " 'Slovak',\n",
       " 'Slovenian',\n",
       " 'Somali',\n",
       " 'Spanish',\n",
       " 'Swahili',\n",
       " 'Swedish',\n",
       " 'Tagalog',\n",
       " 'Tamil',\n",
       " 'Tatar',\n",
       " 'Tegulu',\n",
       " 'Thai',\n",
       " 'Tibetan',\n",
       " 'Turkish',\n",
       " 'Turkmen',\n",
       " 'Ukrainian',\n",
       " 'Urdu',\n",
       " 'Uzbek',\n",
       " 'Vietnamese',\n",
       " 'Welsh',\n",
       " 'Yiddish',\n",
       " 'Yoruba',\n",
       " 'Zulu'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = [entries.split(',') for entries in df.languages_spoken.values if entries!='None']\n",
    "s = set()\n",
    "for l in lol:\n",
    "    for m in l:\n",
    "        s.add(m)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'people_with_language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2ee3b043f3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeople_with_language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'people_with_language'"
     ]
    }
   ],
   "source": [
    "df.people_with_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_with_language = len([v for v in df.languages_spoken.values if v!='None' and v!=''])\n",
    "people_with_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains(',') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilingual and above\n",
    "len(df [ df.languages_spoken.str.contains(',') ].worker_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bilingual'] = df.languages_spoken.str.contains(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_bilingual = df.pivot_table(\n",
    "    index = 'location_country',\n",
    "    columns='bilingual',\n",
    "    values = 'worker_id',\n",
    "    aggfunc='count'\n",
    ").fillna(0)\n",
    "\n",
    "pv_bilingual['perc_bilingual'] = pv_bilingual[True] / (pv_bilingual[True] + pv_bilingual[False])\n",
    "pv_bilingual.sort_values('perc_bilingual', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique ids of workers that answered the language question\n",
    "len(df [ df.languages_spoken !='None' ].worker_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('English') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('Tamil') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('Spanish') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('Hindi') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('French') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('Malayalam') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('Tegulu') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('German') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df [  df.languages_spoken.str.contains('Chinese') ].location_country.value_counts() / df.location_country.value_counts()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [  df.languages_spoken.str.contains('Macedonian') ].location_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = set([l for v in df.languages_spoken.values if v!='None' \n",
    "                 for l in v.split(',') if l!=''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = []\n",
    "for language in languages:\n",
    "    people = len(set(df[ df.languages_spoken.str.contains(language) ].worker_id.values))\n",
    "    result.append({\"lang\":language, \"unique_workers\": people})\n",
    "    \n",
    "\n",
    "df_cnt = pd.DataFrame(result).sort_values('unique_workers', ascending=False)    \n",
    "print (df_cnt)    \n",
    "# more than 10 people for the language\n",
    "\n",
    "#df2 = pd.DataFrame(result).sort_values('unique_workers', ascending=False)\n",
    "#enough = df2 [df2.unique_workers > 9]\n",
    "#len(enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt.unique_workers.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5044"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([v for v in df.languages_spoken.values if 'Spanish' in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10490.450487855513"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1189 / 19268 * 170000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intersect(a, b):\n",
    "    \"\"\" return the intersection of two lists \"\"\"\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "my_langauge = ['English', 'Tamil', 'Spanish', 'Hindi', 'Malayalam', 'French', 'Telugu', 'Chinese', 'German', 'Kannada', 'Italian', 'Portuguese', 'Marathi', 'Arabic', 'Russian', 'Japanese', 'Gujarati', 'Urdu', 'Bengali', 'Punjabi', 'Korean', 'Tagalog', 'Romanian', 'Vietnamese', 'Greek', 'Polish', 'Dutch', 'Turkish', 'Hebrew', 'Swedish', 'Serbian', 'Nepali', 'Bulgarian', 'Macedonian', 'Oriya']\n",
    "pavlick_langauge = ['English', 'Tamil', 'Malayalam', 'Hindi', 'Spanish', 'Telugu', 'Chinese', 'Romanian', 'Portuguese', 'Arabic', 'Kannada', 'German', 'French', 'Polish', 'Urdu', 'Tagalog', 'Marathi', 'Russian', 'Italian', 'Bengali', 'Gujarati', 'Hebrew', 'Dutch', 'Turkish', 'Vietnamese', 'Macedonian', 'Cebuano', 'Swedish', 'Bulgarian', 'Swahili', 'Hungarian', 'Catalan', 'Thai', 'Lithuanian', 'Punjabi']\n",
    "\n",
    "intersect_language = intersect(my_langauge, pavlick_langauge)\n",
    "len(intersect_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.057703081232493, pvalue=0.7419647128381073)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(my_langauge,pavlick_langauge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
